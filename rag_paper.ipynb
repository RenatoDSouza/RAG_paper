{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebf30b58-9685-483b-bd7a-17a0b17c7b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-text-splitters langchain-core langchain-community transformers huggingface-hub torch scikit-learn pypdf sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e30b6245-1c0f-4ea1-91f9-cdff63a7be7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.50.2)\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.local/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.local/lib/python3.10/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in ./.local/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: triton==3.2.0 in ./.local/lib/python3.10/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.10/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf932bc-65a2-4714-bf09-9907a0c1923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, pdf_path: str, hf_token: str):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.hf_token = hf_token\n",
    "        self._load_components()\n",
    "        \n",
    "    def _load_components(self):\n",
    "        \"\"\"Carrega todos os componentes necessários\"\"\"\n",
    "        # 1. Carregar e dividir o PDF\n",
    "        self._load_and_split_documents()\n",
    "        \n",
    "        # 2. Configurar embeddings e vetorstore\n",
    "        self._setup_embeddings()\n",
    "        \n",
    "        # 3. Carregar o modelo Llama 3\n",
    "        self._load_llama_model()\n",
    "        \n",
    "        # 4. Configurar a cadeia RAG\n",
    "        self._setup_rag_chain()\n",
    "    \n",
    "    def _load_and_split_documents(self):\n",
    "        \"\"\"Carrega e pré-processa os documentos\"\"\"\n",
    "        loader = PyPDFLoader(self.pdf_path)\n",
    "        documents = loader.load()\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,  # Aumentado para melhor contexto\n",
    "            chunk_overlap=200,  # Adicionado overlap para manter contexto\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "        self.all_splits = text_splitter.split_documents(documents)\n",
    "    \n",
    "    def _setup_embeddings(self):\n",
    "        \"\"\"Configura os embeddings e o vetorstore\"\"\"\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            model_kwargs={\"device\": \"cuda\"},\n",
    "            encode_kwargs={\"normalize_embeddings\": True}\n",
    "        )\n",
    "        \n",
    "        self.vectorstore = SKLearnVectorStore.from_documents(\n",
    "            documents=self.all_splits,\n",
    "            embedding=self.embeddings,\n",
    "        )\n",
    "        \n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",  # Maximum Marginal Relevance para melhor diversidade\n",
    "            search_kwargs={\n",
    "                \"k\": 5,\n",
    "                \"score_threshold\": 0.5,\n",
    "                \"fetch_k\": 20  # Busca mais documentos inicialmente para melhor seleção\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def _load_llama_model(self):\n",
    "        \"\"\"Carrega o modelo Llama 3\"\"\"\n",
    "        login(token=self.hf_token)\n",
    "        model_name = 'meta-llama/Llama-3.2-3B'\n",
    "        # model_name = 'meta-llama/Llama-3-1B', #\"meta-llama/Meta-Llama-3-8B\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, token=self.hf_token)\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            token=self.hf_token,\n",
    "            torch_dtype=torch.float16, #if torch.cuda.is_available() else torch.float32,\n",
    "            #device_map=\"auto\",\n",
    "            #low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        # Configurar pipeline otimizado\n",
    "        self.pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_new_tokens=300,\n",
    "            temperature=0,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "        )\n",
    "    \n",
    "    def _setup_rag_chain(self):\n",
    "        \"\"\"Configura a cadeia RAG completa\"\"\"\n",
    "        # Template otimizado para Llama 3\n",
    "        prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        You are an expert assistant in the analysis of scientific articles.\n",
    "        Respond based on the documents provided, keeping the information accurate.\n",
    "        \n",
    "        Context: {context}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        Question: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "        Response:\"\"\"\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "        \n",
    "        self.rag_chain = (\n",
    "            {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.pipeline\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    def query(self, question: str) -> str:\n",
    "        \"\"\"Executa uma consulta RAG completa\"\"\"\n",
    "        try:\n",
    "            # Busca e formata os documentos relevantes\n",
    "            docs = self.retriever.invoke(question)\n",
    "            context = \"\\n\\n\".join([f\"Documento {i+1}:\\n{doc.page_content}\" \n",
    "                                 for i, doc in enumerate(docs)])\n",
    "            \n",
    "            # Gera a resposta\n",
    "            response = self.pipeline(\n",
    "                self.prompt.format(context=context, question=question),\n",
    "                max_new_tokens=300,\n",
    "                temperature=0.1\n",
    "            )[0]['generated_text']\n",
    "            \n",
    "            # Extrai apenas a parte da resposta após o último prompt\n",
    "            return response.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].strip()\n",
    "        except Exception as e:\n",
    "            return f\"Erro ao processar a pergunta: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4d19a15-de98-40cd-ba62-b6bd7946dea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_478359/2965555359.py:47: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa91accb35944dcbbe71188bf5ba86d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Uso do pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuração\n",
    "    PDF_PATH = \"Name_of_the_paper\"\n",
    "    HF_TOKEN = \"HF_TOKEN\"\n",
    "    \n",
    "    # Inicialização\n",
    "    rag_app = RAGPipeline(PDF_PATH, HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7126639e-e8c6-474d-9bcb-9af0052b44c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Pergunta: What deposition method was used by the author to make the films for the work?\n",
      "--------------------------------------------------\n",
      "Resposta: Response: Chemical vapour deposition (CVD) is the deposition method used by the author to make the films for the work. CVD is a process that involves the deposition of a material onto a surface using a vaporized precursor. The precursor is typically a gas or liquid that is heated to a high temperature, where it decomposes and forms a thin film on the surface. CVD is a versatile technique that can be used to deposit a wide range of materials, including semiconductors, metals, and insulators. It is commonly used in the semiconductor industry to produce thin films for electronic devices, such as transistors and solar cells. CVD is also used in the production of coatings for optical and mechanical applications. The process is typically carried out in a vacuum chamber, where the precursor is introduced into the chamber and allowed to react with the surface of the substrate. The resulting film is typically uniform and of high quality, making it a popular choice for many applications. CVD is a highly controlled process, allowing for precise control over the thickness, composition, and structure of the deposited film. It is also a relatively fast process, with deposition times ranging from seconds to minutes, depending on the material and the desired thickness. Overall, CVD is a valuable technique for the deposition of thin films, offering a high degree of control and precision, making it a popular choice for many applications in the semiconductor and coatings industries.\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de consulta\n",
    "question = \"What deposition method was used by the author to make the films for the work?\"\n",
    "answer = rag_app.query(question)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Pergunta: {question}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Resposta: {answer}\")\n",
    "print(\"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87f3574-175f-4c51-ab0e-7bf9a476b346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
